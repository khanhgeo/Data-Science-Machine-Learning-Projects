---
title: "DASC5420-Project"
author: "KhanhTV5"
date: "2023-04-15"
output: html_document
---

In this study, I develop housing sale price prediction models using several regression machine learning algorithms, including Linear, Ridge, Lasso, Elastic Net, Principal Component, and Random Forest regressions. This study uses the Ames Housing dataset, which includes information about homes in Ames, Iowa, and has 79 explanatory variables and one target variable, sale price. I perform data cleaning and preprocessing, feature engineering using techniques such as log transformation and feature selection, and evaluate the performance of the different regression models. The results show that the Lasso regression model performs the best in predicting housing sale prices, with an R-squared value of 0.904 and an RMSE of 0.129. This study provides valuable insights into the factors that drive housing prices and demonstrates the potential of machine learning techniques in predicting housing values.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

```{r, results='hide'}
# install and call required library
pkg_list = c("dplyr","tidyverse","ISLR","ISLR2", "caret","ModelMetrics","corrplot", 'ggpubr', 'tidytable',
             'GGally','boot','gridExtra','glmnet','dummy','neuralnet')
# Install packages if needed
for (pkg in pkg_list)
  {# Try loading the library.
    if ( ! library(pkg, logical.return=TRUE, character.only=TRUE) )
      {
        # If the library cannot be loaded, install it; then load.
        install.packages(pkg)
        library(pkg, character.only=TRUE)
      }
  }
```

```{r}
# load datasets
data <- read.csv("../2. Data/data.csv")
```



## Exploratory Data Analysis

```{r}
# get list of column names
names(data)
```
```{r}
# quick view of data structure and data types
glimpse(data)
```

```{r}
# function to get character columns from df
get.char.cols <- function(df) {
  char.cols <- names(select_if(df, is.character))
  return(char.cols)
}
# function to get numeric columns from df
get.numeric.cols <- function(df) {
  num.cols <- names(select_if(df, is.numeric))
  return(num.cols)
}
```

```{r}
# get list of numeric columns
num.cols <- get.numeric.cols(data)

# print summary statistics of numeric variables
summary(data[,num.cols])
```
This show the statistic summary of all numeric variables in the data.

```{r}
# set theme for all visuals of this Rmarkdown
theme_set(theme_classic())
# hist(data.train$SalePrice)
ggplot(data, aes(SalePrice))+
  geom_histogram(fill= "blue", bins = 30)+
  labs(title = "Distribution of Original Sale Price", x = "Sale Price", y = "Frequency")
```

Distribution of housing sale prices is right skewed, showing that most residences were sold for around or less than $200,000, while some were sold for significantly higher prices.

```{r}
# view scatterplot matrix
ggpairs(data[, c("SalePrice", "OverallQual", "GrLivArea", "GarageCars", "GarageArea", "TotalBsmtSF")], lower = list(continuous = "smooth"), diag = list(continuous = "density"))
```

This scatterplot matrix shows the distribution, relationships and correlation between target variable, housing sale prices and several predictors, as well as between predictors.

```{r}
# View boxplots of SalePrice by various categorical variables
p1 <- ggplot(data, aes(x = as.factor(OverallQual), y = SalePrice)) + 
  geom_boxplot(fill = "lightblue", color = "black") + 
  labs(x = "Overall Quality", y = "Sale Price")

p2 <- ggplot(data, aes(x = as.factor(Neighborhood), y = SalePrice)) +
  geom_boxplot(fill = "#69b3a2", color = "black") +
  labs(x = "Neighborhood", y = "Sale Price") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p3 <- ggplot(data, aes(x = as.factor(YearBuilt), y = SalePrice)) +
  geom_boxplot(fill = "#69b3a2", color = "black") +
  labs(x = "Year Built", y = "Sale Price")

grid.arrange(p1, p2, p3, ncol = 1)
```

The boxplots of the sale price by overall quality, neighborhood, and year built showed some significant differences in the median sale prices across these variables.

```{r}
# plot scatterplot of SalePrice and GrLivArea
ggplot(data, aes(x = GrLivArea, y = SalePrice)) + 
  geom_point(color = "darkblue", alpha = 0.5) + 
  labs(title = "Sale Price vs. Above Ground Living Area", x = "Above Ground Living Area", y = "Sale Price")
```

The scatterplot of sale price and above ground living areas shows a positive correlation between these variables.

```{r}
# plot correlation matrix
corrplot(cor(data[, c("SalePrice", "OverallQual", "GrLivArea", "GarageCars", "GarageArea", "TotalBsmtSF")]),method = "circle", order = "hclust", tl.srt = 45)
```

Correlation matrix plot show how strong relationship between housing sale price and several independent variables.



## Data Transformation

```{r}
colSums(is.na(data))
```

```{r}
# copy original data for data transformation
data.clean <- data

# transform response SalePrice into log scale
data.clean$SalePrice <- log(data.clean$SalePrice)

# original SalePrice histogram
p4 <- ggplot(data, aes(SalePrice))+
  geom_histogram(fill= "blue", bins = 30)+
  labs(title = "Original Sale Price Distribution", x = "Sale Price", y = "Frequency")+
  theme_classic()

# transformed SalePrice histogram
p5 <- ggplot(data.clean, aes(SalePrice))+
  geom_histogram(fill= "blue", bins = 30)+
  labs(title = "Log Transformed Sale Price Distribution", x = "Sale Price", y = "Frequency")+
  theme_classic()

grid.arrange(p4, p5, ncol = 2)
```

Comparison of sale price distribution before and after log transformation. Log transformed of sale price has a normal distribution. This transformation help to improve model accuracy of model in predicting sale price.

```{r}
# remove unnecessary variables and predictors that has huge number of missing data
data.clean <- data.clean[, !(names(data.clean) %in% c('Id', 'Utilities', 'Street','Alley', 'FireplaceQu',
                                                      'PoolQC', 'MiscFeature', 'MiscVal'))]
```

```{r}
# Change data types in numerical features that should be categorical
data.clean$MSSubClass <- as.character(data.clean$MSSubClass)
data.clean$YrSold <- as.character(data.clean$YrSold)
data.clean$MoSold <- as.character(data.clean$MoSold)
```

```{r}
# replace corrupted data 
data.clean$Exterior1st <- ifelse(data.clean$Exterior1st == "Wd Sdng", "WdSdng", data.clean$Exterior1st)
data.clean$Exterior2nd <- ifelse(data.clean$Exterior2nd == "Wd Sdng", "WdSdng", data.clean$Exterior2nd)
data.clean$Exterior2nd <- ifelse(data.clean$Exterior2nd == "Wd Shng", "WdShng", data.clean$Exterior2nd)
data.clean$Exterior2nd <- ifelse(data.clean$Exterior2nd == "Brk Cmn", "BrkCmn", data.clean$Exterior2nd)
data.clean$MSZoning <- ifelse(data.clean$MSZoning == "C (all)", "C", data.clean$MSZoning)
```

```{r}
# get list of character and numeric columns
char.cols <- get.char.cols(data.clean)
num.cols <- get.numeric.cols(data.clean)
```

```{r}
# handle missing
data.clean$Electrical <- ifelse(is.na(data.clean$Electrical), "SBrkr", data.clean$Electrical)

# replace character predictors missing values with None
data.clean[char.cols] <- lapply(data.clean[char.cols], function(x) ifelse(is.na(x), "None", x))

# replace remaining numeric predictors missing values with 0
data.clean[num.cols] <- lapply(data.clean[num.cols], function(x) ifelse(is.na(x), 0, x))
```

```{r}
# generate new variable
data.clean$TotalSqrFeet = data.clean$BsmtFinSF1 + data.clean$BsmtFinSF2 + data.clean$X1stFlrSF + data.clean$X2ndFlrSF
```


```{r}
# encode categorical columns
char.cols <- get.char.cols(data.clean)
data.clean <- get_dummies(data.clean, cols = char.cols, prefix = T, prefix_sep = "_", drop_first = T, dummify_na = F)

# cleaned data for models
data.clean <- dplyr::select(data.clean, -char.cols)
```



## Build Models

```{r}
# data train and test split
set.seed(5420)
training.index <- createDataPartition(data.clean$SalePrice, p = 0.70, list = FALSE)
training <- data.clean[training.index, ]
testing <- data.clean[-training.index, ]
```

```{r, eval=FALSE}
set.seed(5420)

# fit Linear regression
lm.model <- train(SalePrice ~ ., data = training, method = "lm",
                  trControl = trainControl("cv", number = 10),
                  tuneLength = 10
                  )

# setup a grid range of lambda values
lambda <- 10^seq(-3, 3, length = 100)

# fit Ridge regression
ridge.model <- train(SalePrice ~., data = training, method = "glmnet",
                     trControl = trainControl("cv", number = 10),
                     tuneGrid = expand.grid(alpha = 0, lambda = lambda)
                     )

# fit Lasso regression:
lasso.model <- train(SalePrice ~., data = training, method = "glmnet",
                     trControl = trainControl("cv", number = 10),
                     tuneGrid = expand.grid(alpha = 1, lambda = lambda)
                     )

# fit Elastic Net regression
elasticNet.model <- train(SalePrice ~., data = training, method = "glmnet",
                          trControl = trainControl("cv", number = 10),
                          tuneLength = 10
                          )

# fit Principal Component Regression
pcr.model <- train(SalePrice ~ ., data = training, method = "pcr", 
                   preProcess = c("center", "scale"), 
                   trControl = trainControl(method = "cv", number = 10), 
                   tuneLength = 10
                   )

# fit Random Forest Regression
rf.model <- train(SalePrice ~ ., data = training, method = "rf",
                  trControl = trainControl("cv", number = 10),
                  tuneLength = 10
                  )


# make predictions on test data
lm.pred <- predict(lm.model, newdata = testing)
ridge.pred <- predict(ridge.model, newdata = testing)
lasso.pred <- predict(lasso.model, newdata = testing)
elasticNet.pred <- predict(elasticNet.model, newdata = testing)
pcr.pred <- predict(pcr.model, newdata = testing)
rf.pred <- predict(rf.model, newdata = testing)


# evaluate the performance of the models using RMSE and R2
results <- data.frame(
  Model = c("Linear", "Ridge", "Lasso",
            "Elastic Net", "PCR", "Random Forest"),
  RMSE = c(caret::RMSE(lm.pred, testing$SalePrice),
           caret::RMSE(ridge.pred, testing$SalePrice),
           caret::RMSE(lasso.pred, testing$SalePrice),
           caret::RMSE(elasticNet.pred, testing$SalePrice),
           caret::RMSE(pcr.pred, testing$SalePrice),
           caret::RMSE(rf.pred, testing$SalePrice)
           ),
  Rsquare = c(caret::R2(lm.pred, testing$SalePrice),
              caret::R2(ridge.pred, testing$SalePrice),
              caret::R2(lasso.pred, testing$SalePrice),
              caret::R2(elasticNet.pred, testing$SalePrice),
              caret::R2(pcr.pred, testing$SalePrice),
              caret::R2(rf.pred, testing$SalePrice)
              )
)

# display results on the same plot 
results$Model <- factor(results$Model, levels = c("Linear","Ridge","Lasso","Elastic Net","PCR","Random Forest"))
p6 <- ggplot(results) +
  geom_point(aes(x = Model, y = RMSE), col = "red", lwd=3)+
  geom_line(aes(x = c(1,2,3,4,5,6), y = RMSE), col = "red", lwd=1)

p7 <- ggplot(results) +
  geom_point(aes(x = Model, y = Rsquare), col = "blue", lwd=3)+
  geom_line(aes(x = c(1,2,3,4,5,6), y = Rsquare), col = "blue", lwd=1)

grid.arrange(p6, p7, ncol = 1)
```
Line plots show the comparison of RMSE and R2 among predictive models on test dataset.

